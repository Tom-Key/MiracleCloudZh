{"config":{"lang":["ja"],"separator":"[\\s\\-\uff0c\u3002]+","pipeline":["stemmer"]},"docs":[{"location":"","title":"\u6b22\u8fce\u6765\u5230 Miracle Cloud","text":"<p>\u672c\u6587\u6863\u7531Miracle Cloud \u82f1\u6587\u6587\u6863\u8fc1\u79fb\u800c\u6765\uff0c\u4e2d\u6587\u76f8\u5173\u5185\u5bb9\u5c1a\u5728\u914d\u7f6e\u4e2d\uff0c\u656c\u8bf7\u671f\u5f85\u3002</p>"},{"location":"OVERVIE/Scenario/","title":"Scenario","text":"<p>Miracle Cloud runs through the entire life cycle of bioinformatics analysis from Preprocessing to Processing to Postprocessing , and can be used for the entire analysis process of bioinformatics data from samples to reports. Miracle Cloud is based on an open and compatible architecture and takes workspace as the core of to help bioinformatics practitioners achieve the goals that analysis process could be restarted, the research results could be reproduced, the operation process could be traced, and the knowledge could be accumulated and disseminated. Typical application scenarios are as follows</p> <ul> <li> <p>Bioinformatics data transmission, storage, management and display</p> </li> <li> <p>Secondary analysis of bioinformatics</p> </li> <li> <p>Third level analysis of bioinformatics</p> </li> <li> <p>Reproduction of typical research papers and books</p> </li> <li> <p>Submission of scientific papers</p> </li> </ul>"},{"location":"OVERVIE/Advantages/Advantages/","title":"Advantages","text":"<p>Protable Process\uff1a Miracle Cloud is based on cloud-native infrastructure and the GA4GH specification standard, which ensures the whole analysis process is portable and conforms to the cloud-native concept \"build once, run everywhere\". Reproducible analysis results\uff1a Miracle Cloud ensures the reproducibility of bioinformatics research by providing functions such as Dashboard, analysis history , notebooks, workflow configuration, etc Shareble Knowedge\uff1a Miracle Cloud provides bioinformatics analysis based on the core function of workspace, which makes the analysis results can be precipitated, disclosed and disseminated in the form of workspace.</p>"},{"location":"OVERVIE/MiracleCloud/Introduction/","title":"Introduction","text":"<p>Miracle cloud bioinformatics analysis cloud platform is a new cloud-native architecture-based biomedical information analysis platform launched by Guangzhou Laboratory and Volcano Engine for bioinformatics practitioners. Miracle Cloud provides core capabilities such as biomedical data transmission, data storage, data management, and data analysis. Miracle Cloud is based on workspace and realizes the whole process of bioinformatics analysis, from Preprossing, Processing to Postprocessing. Miracle Cloud helps users to achieve the goal that analysis process could be restarted, the research results could be reproduced, the operation process could be traced, and the knowledge could be accumulated and disseminated. The platform supports the open standards and specifications of the global genomics and health alliance GA4GH community, and continuously accumulates and precipitates open standard datasets and tool resources, so that users can quickly carry out genetic analysis tasks of any data scale. </p>"},{"location":"OVERVIE/MiracleCloud/Introduction/#architecture","title":"Architecture","text":"<p>The functional architecture of Miracle Cloud is as follows: </p>"},{"location":"OVERVIE/MiracleCloud/Introduction/#features","title":"Features","text":"<p>Bioinformatics Workflow Analysis: Miracle Cloud supports the WES API specification under the Global Genomics and Health Alliance GA4GH. Miracle Cloud has open and compatible architecture, supports popular and standard workflow specifications such as WDL, and also supports large-scale bioinformatics analysis workflow operation. Miracle Cloud has reentrant, reproducible, and portable workflow which help bioinformatics users to start bioinformatics analysis and research quickly, flexibly and conveniently. Bioinformatics Data Management: The data in the Miracle Cloud bioinformatics analysis cloud platform is stored in the \"cloud\", adhering to the concept of unique data storage, and using object storage as a storage medium. The data can be acquired, transmitted and used anywhere and anytime. In addition, the platform provides bioinformatics data Model management sorting, organizing and displaying bioinformatics data in the form of data tables. Bioinformatics Data Management also provides the basis for batch operation of workflow to realize vectorized calculation, and can simultaneously present input data and output results as workflow uniformly. Bioinformatics Data Management is the starting point of workflow and also the end point. Bioinformatics interactive analysis: Miracle Cloud integrates Jupyterhub in interactive analysis . It provides a real-time interactive analysis environment that conforms to the usage behavior of Bioinformatics analyst, and allows Bioinformatics analyst to create and share documents that contain live code, equations, visualizations and narrative text. Multi-cloud heterogeneous environment support: Miracle Cloud bioinformatics analysis cloud platform supports multi-cloud heterogeneous environment management, and can flexibly and conveniently deliver tasks to private cloud environments and public cloud environments according to type and magnitude of tasks. It also supports delivery of tasks to HPC Clusters or Cloud Clusters according to user need, which brings the benefit of maximizing resource utilization and flexibility.</p>"},{"location":"TUTORIALS/QuickStart/QuikstartDataManagement/","title":"QuikstartDataManagement","text":""},{"location":"TUTORIALS/QuickStart/QuikstartDataManagement/#differences-in-data-management-between-cloud-and-traditional-bioinformatics-analytics","title":"Differences in data management between cloud and traditional bioinformatics analytics","text":"<p>In traditional biological information research, users need to download data from the public standard bioinformatics database to be stored locally, while in the cloud-based bioinformatics analysis platform, the data is always stored in the cloud and users are able to access the data as link anytime, anywhere. To obtain and use data, users do not need to download and store, which avoids the wastage of time, transmission errors and storage costs. </p>"},{"location":"TUTORIALS/QuickStart/QuikstartDataManagement/#where-is-miracle-cloud-data-stored","title":"Where is Miracle Cloud data stored","text":"<p>Compared with the traditional data usage method that needs to copy data to the local, the usage method of cloud scenarios may not be intuitive. When discussing the data in the workspace in the Miracle Cloud, In addition to the data stored in the corresponding bucket of your Workspace, it is also common to link data to your Workspace in the form of a link (which does not actually exist in the corresponding bucket of your Workspace). In fact, when analyzing sample data, except that the sample data needs to be uploaded, while the other data does not need to be copied to your Workspace bucket. All analysis is done in the cloud, and only the generated data will be stored in the Workspace storage in the bucket. Typically, data analyzed in Miracle Cloud will be located in the following three locations: </p> <ul> <li>Workspace bucket: Typically, personal data you upload (such as sample data), output data from workflow output, and ipynb files from notebooks are stored in the Workspace bucket</li> <li>Outside Workspace Bucket : Most of the data you use will be stored in some other data store in the cloud (like the publicly reference genome data provided by Miracle Cloud), which you can access and use in Miracle Cloud as long as you have the correct permissions and authorizations.</li> <li>Container Cluster: When using the Notebook module for interactive analysis, the generated output data is stored by default in the persistent storage PVC of the container cluster. Additionally, if you need to share this data with others, you can copy it to a Workspace bucket</li> </ul>"},{"location":"TUTORIALS/QuickStart/QuikstartDataManagement/#advantages-of-data-models","title":"Advantages of Data Models","text":"<p>Since Miracle Cloud provides buckets for storing and sharing data, why does it provide a data model for organizing data? That's because although it takes some time to follow certain rules to make data tables initially, the advantages of data models will emerge as the amount of data grows:</p> <ul> <li> <p>Ability to organize, display and manage tens of thousands of samples for easy tracking, browsing and sorting</p> </li> <li> <p>Ability to easily perform parallel computing. By selecting the data entity in the data model, it can be easily specified as the input of the workflow. The workflow of analyzing 1 sample and analyzing 1000 samples is almost the same, which makes it very good scalability.</p> </li> <li> <p>Ability to write output data back to the data model by setting workflow output parameters, which will automatically associate output results from input data</p> </li> <li> <p>Writes output back to the data model that can be used as input to the next run without having to find the file path in the workspace bucket</p> </li> </ul>"},{"location":"TUTORIALS/QuickStart/QuikstartWorkspace/","title":"QuikstartWorkspace","text":""},{"location":"TUTORIALS/QuickStart/QuikstartWorkspace/#biomedical-research-in-workscpace","title":"Biomedical Research in Workscpace","text":""},{"location":"TUTORIALS/QuickStart/QuikstartWorkspace/#workspace-concept","title":"Workspace Concept","text":"<p>In Miracle Cloud platform, workspace is a complete encapsulation of a bioinformatics research process, including data, environment, code, operational calculation procedures, results, and dashboard as an overview. It is the basic unit that realizes executable, transportable, reproducible, shareable and publishable scientific research and biological application. </p> <ul> <li> <p>Bioinformatics Dataset Management: The data in Miracle Cloud is stored in the cloud, and users can use it directly in the form of links instead of downloading it to local storage, thereby saving transmission time and storage costs.</p> </li> <li> <p>Bioinformatics data model: Data stored in different locations in the cloud can be effectively organized and displayed in the form of data tables, which can be used as the basis for vectorized calculations, and the calculation results can also be written back to the data table.</p> </li> <li> <p>Workflow: Support flexible and convenient workflow for bioinformatics analysis tasks of any size</p> </li> <li> <p>Interactive Analysis: Providing a real-time interactive analysis environment that conforms to the usage habits of bioinformatics practitioners, it can help users visualize and analyze data of any scale</p> </li> <li> <p>Dashboard: As an overall introduction to all research done in Workspace, it's able to record information such as data resources, workflow resources, and operation steps in the research process, and can also reproduce research in Workspace through the available interactive analysis environment.</p> </li> <li> <p>Cloud Native Scheduling: Support container clusters as back-end computing resources to ensure resource utilization, and at the same time ensure a unified and reproducible environment</p> </li> <li> <p>GA4GH Open Standards Support:  Fully support the GA4GH open standard from the level of data and tools, and integrate into the community ecology</p> </li> </ul>"},{"location":"TUTORIALS/QuickStart/QuikstartWorkspace/#overview-of-workspace-features","title":"Overview of Workspace Features","text":""},{"location":"TUTORIALS/QuickStart/QuikstartWorkspace/#record-and-reproduce-research-process-in-dashboard","title":"Record and Reproduce research process in Dashboard","text":"<p>Dashboard is integrated with Jupyterhub. On one hand, as an overview, the entire process of research in Workspace can be recorded through Markdown documents, including data, workflow, operation steps and results, etc., which can easily and quickly let others understand all the research content in this workspace. On the other hand, by providing a real-time interactive analysis environment, it can call the API of notebook and workflow which can quickly reproduce the work in the workspace.  In addition, the basic information module in the Dashboard will display the name, description, creation time, and administrator information of the Workspace\uff0cand will also display the storage information corresponding to the Workspace. </p>"},{"location":"TUTORIALS/QuickStart/QuikstartWorkspace/#upload-and-store-data-in-a-bucket","title":"Upload and store data in a bucket","text":"<p>Each Workspace in Miracle Cloud corresponds to a bucket. This bucket information can be viewed from environmental information. The bucket corresponding to the Workspace can store the following contents:</p> <ul> <li> <p>User's own sample data which can be uploaded through pages, command lines, etc</p> </li> <li> <p>Output results, logs and other information during workflow operation</p> </li> <li> <p>Notebook file such as <code>.ipynb</code> file</p> </li> </ul> <p>Note: The data generated in the Notebook will not be stored in the bucket, but will be stored locally on the server where the Notebook is located by default. If necessary, the data needs to be manually copied to the bucket.</p>"},{"location":"TUTORIALS/QuickStart/QuikstartWorkspace/#organize-and-manage-data-in-data-modules","title":"Organize and manage data in data modules","text":"<p>Miracle Cloud provides data model to organize, display and manage data in the form of data tables on the platform:</p> <ul> <li> <p>Use the user's sample data as a data table and as the input of the workflow, which simplifies the batch operation of task delivery.</p> </li> <li> <p>The output result data of the workflow can be written back to the table. On the one hand, it realizes the unified display of input and output data, and on the other hand, the output data can be used as the input for the next step.</p> </li> <li> <p>Each row in the entity data table is represented as an entity, and each entity has a unique entity ID (first column of data); Each column in the entity data table is the attribute value of each entity.</p> </li> <li> <p>Entity Data Model Entity data model can be divided into entity data table and entity data set table. In the entity data table, the entity ID in the first column corresponds to a data entity, and the data entity is linked to the data set information actually stored in the cloud in the form of a link. In the entity data set table, the entity set ID in the first column corresponds to a set of data entities, and the content of the set is an array composed of data entity IDs. Note:</p> </li> <li>Entity data set table and entity data table is one-to-one correspondence</li> <li>The entity collection table itself does not have a data entity link, but will index the data in the entity data table with the column name.  </li> <li>Workspace data model The Workspace-level data model, as the name suggests, means that the content in this data table is the data required by all sample data in the entire Workspace, such as reference genomic data, Docker images url, etc. </li> <li>Data file Management</li> </ul> <p>File lists are some file data stored in a bucket that can be inserted into a notebook for reading or linked to an entity data model.  </p>"},{"location":"TUTORIALS/QuickStart/QuikstartWorkspace/#interactive-analysis-using-notebooks","title":"Interactive Analysis using Notebooks","text":"<p>Miracle Cloud integrates Jupyterhub open source components to provide users with an environment for real-time interactive analysis of bioinformatics data.</p>"},{"location":"TUTORIALS/QuickStart/QuikstartWorkspace/#using-workflows-to-simplify-bioinformatics-batch-analysis","title":"Using Workflows to Simplify Bioinformatics Batch Analysis","text":"<p>The workflow module can provide flexible and convenient workflow operation such as import, viewing, configuration, and delivery. In addition, it can be used in combination with the data model to quickly organize the input data and output data of the workflow to achieve efficient management of bioinformatics data.</p>"},{"location":"TUTORIALS/QuickStart/QuikstartWorkspace/#view-the-task-analysis-process-and-troubleshooting-in-job-history","title":"View the task analysis process and troubleshooting in Job History","text":"<p>The Miracle Cloud analysis history page is a historical record of selecting a certain workflow for analysis of sample data. One delivery by the user is regarded as the analysis of one task. The analysis of one task includes batch analysis of multiple samples (if there are multiple workflows running in batches). The operation of each workflow may be split into multiple steps. In the Job history, you can accurately view the history records of delivery, workflow operation, and task operation.  You could do troubleshooting based on task level logs and workflow level logs when the analysis process goes wrong. In addition, the configuration information of this workflow operation (including input sample data and parameter configuration) can be recorded in the analysis history, and the analysis can be quickly reproduced by clicking to jump directly to the configuration page at that time.</p>"},{"location":"TUTORIALS/UserGuide/DataSets/","title":"DataSets","text":"<p>DataSets</p>"},{"location":"TUTORIALS/UserGuide/Digger/","title":"Digger","text":"<p>Digger's full name is Digital Gallery, which is a mature, public workspace. Users can publish the complete Workspace to Digger and share it with external person.</p>"},{"location":"TUTORIALS/UserGuide/Digger/#publish-workspace-to-digger","title":"Publish Workspace to Digger","text":"<ol> <li>Select a workspace, click the Options button, click Publish. </li> </ol> <ol> <li>Enter name, publish authorization code, and a short description, and click OK. You can choose to enable controlled release in More settings. After enabling controlled release, the published Workspace will only appear in your Digger list, not in the Digger list of other users (including sub-accounts and master accounts). </li> </ol>"},{"location":"TUTORIALS/UserGuide/Digger/#clone-workspace-in-your-digger","title":"Clone workspace in your digger","text":"<p>Miracle Cloud allows you to fork the Workspace in Digger as a personal Workspace, and you can carry out research reproduction and future work based on the previous work. </p>"},{"location":"TUTORIALS/UserGuide/Digger/#share-workspace-in-your-digger","title":"Share workspace in your digger","text":"<p>Miracle Cloud allows you to share the Workspace in Digger with others. You can click the options button on Workspace and click the share button. Of course, your sharing will be read-only, and others cannot edit or run the Workspace after opening the link. </p>"},{"location":"TUTORIALS/UserGuide/Dockstore/","title":"Dockstore","text":"<p>Dockstore</p>"},{"location":"TUTORIALS/UserGuide/GitLab/","title":"GitLab","text":"<p>GitLab</p>"},{"location":"TUTORIALS/UserGuide/Workspace/","title":"Workspace","text":"<p>A workspace is a computational sandbox created by a user on Miracle Cloud, which contains data, workflow, real-time interactive analysis environment and analysis results. Usually it is a complete project. The workspace can be used as a basic unit for data management, resource allocation, workflow configuration and paper delivery.</p>"},{"location":"TUTORIALS/UserGuide/Workspace/#workspace-introduction","title":"Workspace Introduction","text":"<p>Workspace contains six parts: Dashboard, Data, Notebook, Workflow, Analysis History, and Environment Management. Dashboard:  As an overall introduction to all the research done in Workspace, it can record information such as data resources, workflow resources and operation steps during the research process, and can also reproduce the research in Workspace through the interactive analysis environment. Data:  Manage Bioinformatics data and data models, this is where you work begin. Notebooks: Based on jupyter notebook, it help users visualize and analyze data of any scale by providing a real-time interactive analysis environment that meets the usage habits of Bioinformatical practitioners. Workflows: Using flexible pipeline for batch processing genomics data. Job history:  Record delivery analysis, workflow operation and multi-level task operation history Environment: Demonstrate information on currently used resource pools. </p>"},{"location":"TUTORIALS/UserGuide/Workspace/#create-new-workspace","title":"Create New Workspace","text":"<p>Click Workspace in the left sidebar and select New workspace. There are two options for creating a new workspace. - Create a blank Workspace: Create a blank workspace, enter name and short description - Copy from Digger: Digger is a private workspace repository, you can copy and modify the workspace in Digger. Name: It consists of Chinese, numbers, letters, and hyphens (-\u3001_). It cannot start with a hyphen (-\u3001_), within 1 to 200 characters. Short description: The short description cannot be empty and cannot exceed 1000 characters. </p>"},{"location":"TUTORIALS/UserGuide/Workspace/Dashboard/","title":"Dashboard","text":"<p>The Dashboard contains basic information about the current workspace such as name, short description, creation time, and administrator, as well as the environment space such as compute resource pool, compute type, and bucket. Click the Edit button in the upper right corner to enter the Jupyter Notebook editing interface, where you can write the overview of the workspace using markdown . Of course, you can also use python or R language for interactive analysis, etc.   Click save button after finishing editing the dashboard.</p>"},{"location":"TUTORIALS/UserGuide/Workspace/Data/","title":"Data","text":"<p>The data in Miracle Cloud is stored in the cloud-native platform, and users can use it directly in the form of a link instead of downloading to local storage, thereby saving transmission time and storage costs. There are three parts of the data: Entity Data Model, Workspace Data Model and Data files.</p>"},{"location":"TUTORIALS/UserGuide/Workspace/Data/#entity-data-model","title":"Entity Data Model","text":"<p>The Entity Data Model is to organize, manage and display the information data in the form of data tables, and also provide the basis for the batch operation of the workflow to complete the bioinformatics workflow analysis.</p>"},{"location":"TUTORIALS/UserGuide/Workspace/Data/#create-new-data-model","title":"Create New Data Model","text":"<ol> <li> <p>Click Data on the left panel of workspace and then click the blue + button on the right side of the entity data model, \"Import entity table\" pop-up appears. </p> </li> <li> <p>Here you can download the CSV file template and edit the data . The template should contain at least one entity line. You could upload the CSV file after editing, just drag and drop to complete the file uploading.</p> </li> <li> <p>Finally, click Import Data Table to complete the import of the data model. </p> </li> </ol>"},{"location":"TUTORIALS/UserGuide/Workspace/Data/#generate-entity-set","title":"Generate Entity Set","text":"<p>Generating entity set is used to combine two data rows to generate a new entity set, without creating the array content by the user himself. </p> <ol> <li> <p>On the Entity Data Model page, select at least two entities and click Generate entity collection</p> </li> <li> <p>Input the Entity Collection ID and click OK. </p> </li> </ol> <p>The Default ID is: Entity set name _set-year-month-day-hour-minute-second </p>"},{"location":"TUTORIALS/UserGuide/Workspace/Data/#workspace-data-model","title":"Workspace Data Model","text":"<p>Workspace Data Model is public data required by different workflows in the entire Workspace, and it is no longer necessary to attach these common resources for each sample in the entity table, such as public reference data, Docker images url etc. For example, you can upload data such as reference genomes data to the Workspace data model. Click Workspace Data - Import, and then drag and drop the edited CSV file to import data table. You can also download and delete imported files.</p>"},{"location":"TUTORIALS/UserGuide/Workspace/Data/#data-file-management","title":"Data File Management","text":"<p>Click Data on the left menu, select Data file Management - File Lists - Upload File/Upload Folder, support users to drag and drop to upload or click Select Local File Upload. In addition to web page uploading, Miracle Cloud also supports uploading, downloading and querying files through the CLI command line.</p> <p>Note: When uploading a folder, all files in the folder directory will be uploaded to the current directory. The folder supports uploading up to 2000 files, and the maximum limit of each file is 2G. It is recommended to use the CLI tool for uploading and downloading large files</p> <p>Attention\uff1aif you want to use file or image from file list in Notebook\uff0cyou need to change the file permission to Public Read . Please check the operation below. </p> <p>CLI file upload </p> <ol> <li>Install CLI tool</li> </ol> <p>Miraclecloud CLI is a tool for uploading and downloading files to TOS of Miracle Cloud. You could use Terminal to install in environments that require upload or download operations, such as PC.</p> <pre><code>pip install -i https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple miraclecloud==0.0.10\n</code></pre> <p>If volcengine or tos dependencies are missing after execution, please install volcengine and tos first. Volcengine and tos are the official sdk of Volcano Engine. You need to install it first. Execute the command in termianl as follows:</p> <pre><code>pip install volcengine\npip install tos\n</code></pre> <ol> <li>Configure the environment</li> </ol> <p>In this step, we need to obtain four environment parameters: AccessKey ID, AccessKey Secret, Miracle IP address and Workspace ID.</p> <ul> <li>Get Miracle Cloud Access Key</li> </ul> <p>Log in to the console, click profile photo in the upper right corner of the page, select Access Control in the drop-down menu, and click Key Management on the left Click Create Key and note the AccessKey ID and AccessKey Secret content that pops up on the page.</p> <ul> <li> <p>If you login account is sub account, you need to additionally configure key authorization for sub account.</p> </li> <li> <p>Key usage authorization configuration method:</p> <p>Only sub accounts require this step</p> <ul> <li> <p>Log in to the miracle cloud console (referred as console below) with the main account.</p> </li> <li> <p>Click on the profile photo in the upper right corner of the page, select Access Control in the drop-down menu.</p> </li> <li> <p>Click Policy Management on the left, then click Create Policy, enter the following content and click OK:</p> </li> </ul> <p><code>{   \"Statement\": [     {       \"Effect\": \"Allow\",       \"Action\": [         \"iam:*AccessKey\",         \"iam:ListAccessKeys\",         \"iam:*SecretKey\",         \"iam:ListSecretKeys\"       ],       \"Resource\": [         \"*\"       ]     }   ] }</code></p> <ul> <li> <p>Click User Management on the left, find the sub account in the user list, and click Association Policy</p> </li> <li> <p>Then click Add Policy, find AccessKeyFullAccess, tick the Optional Policy and move to Selected Policy, and finally click OK button</p> </li> </ul> </li> <li> <p>Get Miracle Cloud ip address and Workspace ID</p> </li> </ul> <p>Click to enter the notebook of the current Workspace, select Edit the current notebook or create a new notebook, execute the following code in the notebook cell, get the Miracle IP address and Workspace ID and record them.</p> <pre><code>import os\n# \u8f93\u51faMiracle ip-adress\nos.environ['MIRACLE\\_ENDPOINT']  \n# \u8f93\u51faWorkspace id\nos.environ['JUPYTERHUB_SERVER_NAME']\n</code></pre> <p></p> <ol> <li>Configure environment parameters</li> </ol> <p>After obtaining the four environment variables in the previous step, run the following four command lines in the terminal on your environment, replacing the corresponding parameters with the parameters just obtained</p> <pre><code>export JUPYTERHUB_SERVER_NAME=wc*****************0g\nexport MIRACLE_ENDPOINT=http://*********\nexport MIRACLE_ACCESS_KEY=AKLT**************************************NiNjM\nexport MIRACLE_SECRET_KEY=WVdV**************************************************5XWQ==\n</code></pre> <p></p> <ol> <li> <p>Use the cli command to upload and download file to TOS of Miracle Cloud</p> </li> <li> <p>Query file list</p> </li> </ol> <p>Returns all files in the target path directory, the number of files, and the file size.</p> <pre><code>#For ex, list all files under root directory\uff1a miraclecloud.cli list /\nmiraclecloud.cli list &lt;target_path&gt; &lt;-options&gt;\n</code></pre> Option Parameter description If required -n or --num Set the number of queries this time, the default is 0, means all queries NO \\ <ul> <li>Upload file</li> </ul> <p>Upload file to target directory</p> <pre><code>miraclecloud.cli upload &lt;local_path&gt; &lt;target_path&gt; &lt;-options&gt;\n</code></pre> Option Parameter description If required -r or --recursive For uploading file folder NO --ignore  According to the regex, a certain type of file can be ignored\uff0csuch as .*txt , .*doc NO --include  According to the regex, a certain type of file can be specified\uff0csuch as .*txt , .*doc NO <p>Example: To upload the genomics.ipynb files in the local Download folder to the notebook folder under root directory:</p> <pre><code>miraclecloud.cli upload ~/Downloads/genomics.ipynb notebook\n</code></pre> <p> After the upload is successful, the file appears in the notebook folder:  - Download file</p> <pre><code>miraclecloud.cli download &lt;target_path&gt; &lt;local_path&gt; &lt;-options&gt;\n</code></pre> Option Parameter Description If required -r or --recursive For download folder NO -f or --force If the option is not input, then if there is a file with the same name, the download will be skipped. If the option is input, the file with same name will be overwritten. NO --ignore  According to the regex, a certain type of file can be ignored\uff0csuch as .*txt , .*doc NO --include  According to the regex, a certain type of file can be specified\uff0csuch as .*txt , .*doc NO <ul> <li>Delete file</li> </ul> <pre><code>miraclecloud.cli delete &lt;target_path&gt; &lt;-options&gt;\n</code></pre> \u9009\u9879 \u53c2\u6570\u8bf4\u660e If required -r or --recursive For delete file folder NO --ignore  According to the regex, a certain type of file can be ignored\uff0csuch as .*txt , .*doc NO --include  According to the regex, a certain type of file can be specified\uff0csuch as .*txt , .*doc NO"},{"location":"TUTORIALS/UserGuide/Workspace/Environment/","title":"Environment","text":"<p>In Miracle Cloud, you can choose a cluster for the workflow or Notebook in the current workspace. Click on the environmental management in the left menu to see the current resource information. Workflows and NoteBooks can only be associated with a cluster. After the association, the workflow of the Workspace workflow will be delivered to the affiliated cluster, and Notebook will also start from the affiliated cluster. Workflows and NOTEBOOK can choose to associate with different clusters. The replacement cluster needs to be lifted and re-associated the new cluster. </p>"},{"location":"TUTORIALS/UserGuide/Workspace/JobHistory/","title":"JobHistory","text":"<p>All workflow running records for this workspace are recorded in the job history. The recorded information includes: delivery name, analysis status, number of data entities, start time and analysis time.</p> <ol> <li> <p>Click Job History on the left menu of the workspace, select any of the analysis job, you could view details of the current analysis, including analysis status, delivery overview, and a list of workflow runs.</p> </li> <li> <p>You could click workflow configuration to see the workflow configuration such as input, outpu for this analysis job.</p> </li> </ol>"},{"location":"TUTORIALS/UserGuide/Workspace/Member/","title":"Member","text":"<p>Dashboard.md Data.md Digger.md Environment.md FiveMinuteIntro.md InteractiveAnalysis.md JobHistory.md Workflow.md Workspace.md</p>"},{"location":"TUTORIALS/UserGuide/Workspace/Notebooks/","title":"Notebooks","text":"<p>Miracle Cloud's interactive analytics environment integrates with Jupyter Notebook. Jupyter Notebook is a shareable, replicable analysis. Jupyter Notebook is an open source analytics environment where you could get real-time insight into research data through interactive analysis and visualizations. You can import data - including genomics or transcriptomic data stored in the cloud - and analyze it using custom or pre-built libraries in R or Python. The Jupyter Notebooks provide environment for newbies and it is portable and reproducible. Notebooks combine analytical methods and visualization in one place in an easy-to-understand and shareable way. As a logical evolution of traditional scientific papers, Jupyter Notebooks dramatically shorten the path between reading how analysis is done and actually reproducing it. It's hard to overstate how powerful this concept is and the impact notebooks have on the reusability and reproducibility in computational science.</p> <p></p>"},{"location":"TUTORIALS/UserGuide/Workspace/Notebooks/#create-a-new-notebook","title":"Create a new Notebook","text":"<ol> <li>Click New Notebook, enter a name and select a language. Currently supported languages are Python and R.</li> </ol> <ol> <li>Click the edit button on the right to edit the notebook.</li> </ol> <ol> <li> <p>There are three ways to run code in a cell:</p> <ol> <li> <p>Select the cell and press Shift + Enter on your keyboard (your keyboard may show \"return\" instead of \"enter\").</p> </li> <li> <p>Click the Run icon in the menu bar.</p> </li> <li> <p>Use the appropriate command from the Cell drop-down menu.</p> </li> </ol> </li> </ol> <p>Cells are part of the Notebook. Each cell has a \"type\" (Code/Markdown/Raw NBConvert/Heading) that determines how the application computation will interpret the instructions in the cell.</p> <p></p>"},{"location":"TUTORIALS/UserGuide/Workspace/Notebooks/#how-does-code-cell-work","title":"How does code cell work?","text":"<p>When you run a code cell, the Jupyter compute kernel reads the code, passes those code to the actual operating system running Jupyter (eg Python, R), and retrieves the results to display them in a notebook. When the command runs, the output log of the command appears directly below the code cell. If the code in the cell does not match the language of the kernel, the application calculation will return an error. If no output is specified, you will know that the code executed successfully by paying attention to the number in square brackets [] on the left side of the cell. </p>"},{"location":"TUTORIALS/UserGuide/Workspace/Notebooks/#how-to-know-if-cell-is-executing","title":"How to know if cell is executing?","text":"<p>When starting the notebook for the first time, the square brackets to the left of each cell are empty [], indicating that those units have not been run during this session. An asterisk in parentheses [*] indicates that the unit is running. Once a command is executed, the asterisk is replaced by an integer representing the number of commands executed since the kernel started. You can execute the same cell multiple times, or execute multiple commands in one cell.  If you clear the output by going to the drop-down menu Cells &gt; All Outputs and selecting Clear, the integer brackets will again be replaced by empty brackets. However, if you restart the kernel, the integer count will only reset to zero. </p>"},{"location":"TUTORIALS/UserGuide/Workspace/Notebooks/#how-to-edit-content-in-markdown-cells","title":"How to edit content in Markdown cells","text":"<p>Notebook can not only run python or R language code, but also supports editing content in Markdown format. First, please switch the cell type to Markdown, as shown in the figure below. Then please double-click the cell you want to edit.</p> <p>Markdown is a lightweight plain text formatting language. It replaces typesetting with concise syntax, unlike the general word processing software Word or Pages that we use with a lot of typesetting and font settings. Here are some syntax examples: In Markdown, if a piece of text is defined as a title, just prefix the text with a \"#\" .  Example: # first-level title If you need to quote a short sentence from elsewhere, use the quoted format. Just add angle brackets (&gt;) before the text. The syntax of inserting a link is very similar to inserting a picture, the difference is a \"! \"sign Image example\uff1a![](){ImgCap}{/ImgCap} Link example\uff1a[]()</p>"},{"location":"TUTORIALS/UserGuide/Workspace/Workflow/","title":"Workflow","text":"<p>Workflow is the biological information analysis process, Miracle Cloud's analysis process supports WDL description language. On the workflow page, you can choose custom import or Import from the workflow repository . In the workflow repository you can find the bioinformatics analysis process for batch processing genomics data. After import, computational analysis can be initiated. </p>"},{"location":"TUTORIALS/UserGuide/Workspace/Workflow/#custom-import","title":"Custom Import","text":"<p>Select Custom Import, enter workflow name, git address, git project tag and token, main workflow path, short description. Then click OK when done.  Git tag: In git, tags are used to specify a specific commit. Take gitlab as an example, select the branch to see the current tag you need.  Git token:  Gitlab removed support for password authentication on 2021.8.13, it recommends using personal access token instead of password authentication, you can find Access Token in the settings on the left menu and copy it into the parameters.  Main workflow path: You could find the WDL file you need to import the workflow, and click the Copy button to directly copy the path of the current file into the input parameters.  CallCaching: When enabled, the cache of previously run tasks is searched for tasks with the exact same command and the exact same input. If the cache hits, the results of the previous task will be used instead of being re-run, saving time and resources.</p>"},{"location":"TUTORIALS/UserGuide/Workspace/Workflow/#import-from-workflow-repository","title":"Import from workflow repository","text":"<p>This function is still under development...</p>"},{"location":"TUTORIALS/UserGuide/Workspace/Workflow/#configure-workflow-inputs","title":"Configure workflow inputs","text":"<p>After creating a workflow , to run a workflow on Miracle Cloud, you need to specify all required workflow input variables.</p> <ol> <li> <p>Specify Entity Data Model, and click Select Data. CallCaching: When enabled, the cache of previously run tasks is searched for tasks with the exact same command and the exact same input. If the cache hits, the results of the previous task will be used instead of being re-run, saving time and resources. </p> </li> <li> <p>Configure input parameters Click the Input Parameters tab to configure input attribute values. Attributes are integer, string, or file types that correspond to input variables in the workflow. You should specify input parameters by filling in all required variables in the attribute value column. You could manually input or use this to specify the input parameters in entity data model. Or you could upload json file to input the attribute value.  Examples of some common attribute value formats: Int - <code>50</code>  String - <code>string</code> Array\uff08Separated by commas\uff09 - <code>[string1,string2]</code> File Path - <code>s3://analysis /sc94ra7leig43voqu4vlg /CramToBamFlow /5d915601-75c3-493b-9c7d-d136c9f9a296 /call-CramToBamTask /execution/script</code></p> </li> </ol>"},{"location":"TUTORIALS/UserGuide/Workspace/Workflow/#how-to-write-metadata-of-output-file-to-input-data-table","title":"How to write metadata of output file to input data table","text":"<p>Why we need to write output to a data table? Writing output back to data table will associate the output of a workflow with an input data file which helps organize the output in a meaningful way. It also makes it easy to use the data for next step analysis.</p> <ol> <li>Go to the Output Parameters tab.</li> <li>For each output variable, click the Attribute Value field. You will see a drop-down list of all columns that exist in the selected entity data model.</li> <li>Select an existing column or enter a new name to add a new data column to the data model. If there is no such column in the original data model, a new column will be added according to the column name; if the original data model has a corresponding column with same name, the new result will be populated or overwritten directly. </li> </ol>"},{"location":"TUTORIALS/UserGuide/Workspace/Workflow/#use-json-to-set-input-and-output-parameters","title":"Use Json to set input and output parameters","text":"<p>What is JSON? JSON is a text-based, readable file that stores simple data structures and objects in JavaScript Object Notation (JSON) format. The open standard JSON format transfers WDL configuration to a server running WDL. JSON files can be edited using a text editor.</p> <p>When clicking the Input Parameters or Output Parameters tab, you can choose to download the JSON file template. After editing, you can upload the JSON file to set the workflow input or output.  For different workflows, it comes in handy when you want to use the same input in your workflow configuration. For example, if you create a new configuration, usually you have to manually enter all the inputs every time, even though this new configuration will use many of the same inputs as the existing configuration. You can now download the input from the previous configuration as JSON and upload it to fill into the new configuration.</p>"},{"location":"TUTORIALS/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/GenomeanalysisusingGATK/","title":"GenomeanalysisusingGATK","text":""},{"location":"TUTORIALS/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/GenomeanalysisusingGATK/#genomic-analyses-using-gatk-workflow","title":"Genomic analyses Using GATK Workflow","text":"<p>This chapter describes how to use the Genomic Analysis Toolkit (GATK) to run a secondary genomic analysis workflow on Miracle Cloud. The tools used in this chapter is GATK, which is used to convert the sequence in Cram format to Bam format and then perform mutation analysis to obtain the mutation intermediate result gvcf file. The workflow is written in WDL and runs on the Cromwell.</p>"},{"location":"TUTORIALS/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/GenomeanalysisusingGATK/#gatk-introduction","title":"GATK Introduction","text":"<p>GATK is the abbreviation of Genome Analysis Toolkit, which is a set of software and toolkit used to process high-throughput sequencing data. Originally, GATK was designed to analyze the human genome and exons, mainly to find SNPs and indels. Later, the functions of GATK are becoming more and more abundant, adding new functions such as short variant calling, Copy number variation(CNV) and structural variation (SV). At the same time, GATK is also increasingly used in data analytics of other species. Now, GATK has become the industry standard for finding variants during genomic and RNA-seq analysis.</p>"},{"location":"TUTORIALS/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/GenomeanalysisusingGATK/#part-1-running-cram2bam-data-format-conversion-workflow","title":"Part 1: Running cram2Bam data format conversion workflow","text":"<p>In this section you can learn how to upload data and run the workflow successfully. This workflow implements file format conversion from cram to bam. 1. #### Login to Miracle Cloud Using the account and password to login to Miracle Cloud. The account and password can be obtained by contacting the administrator. There are three menus at the top of the page: Overview, Workspace and Digger. Digger is a public Workspace. Usually, you can choose to publish the completed Workspace to Digger, or you can copy the Workspace in Digger back to Workspace for re-editing.Select Workspace, click New Workspace, entering name: GATK-Workflow and a short description. After creating the Workspace we will start building the Workspace. </p> <pre><code># Dataset download links used in this tutorial\uff1a\n# DataModel(CSV):\nhttps://tutorials-data.tos-cn-guangzhou.volces.com/cram-to-bam/sample.csv\n#sample-datasets\uff1a\nhttps://tutorials-data.tos-cn-guangzhou.volces.com/cram-to-bam/sample-data/NA12878.cram\nhttps://tutorials-data.tos-cn-guangzhou.volces.com/cram-to-bam/sample-data/my-sample-data.cram\n#reference-datasets\uff1a\nhttps://tutorials-data.tos-cn-guangzhou.volces.com/cram-to-bam/reference-data/Homo\\_sapiens\\_assembly38.dict\nhttps://tutorials-data.tos-cn-guangzhou.volces.com/cram-to-bam/reference-data/Homo\\_sapiens\\_assembly38.fasta\n*https://tutorials-data.tos-cn-guangzhou.volces.com/cram-to-bam/reference-data/Homo\\_sapiens\\_assembly38.fasta.fai\n</code></pre> <p>Click Data-File List on the left sidebar of the platform to enter the file data management page. By clicking Upload File, you can upload the local data file to the object storage bucket corresponding to this Workspae according to the guidance.   After the data is uploaded, download the csv template and organize the data. Click the data column on the left, click the \"+\" on the right side of the entity data model list, the upload data model option box appears, and download the data model template. After the data is uploaded, download the csv template and organize the data. Click the data column on the left, click the \"+\" on the right side of the entity data model list, the upload data model option box appears, and download the data model template. </p> <p>The sample_id column is the unique ID of the sample file, and the other columns are attribute columns. In the attribute column, the attribute can be a specific integer or string. Of course, more often, the attribute column will be associated with the data file on the cloud. Here we use the data model to associate the uploaded dataset files. a. First, you need to enter the Data and the File lists; b. Clicking the link label behind the file, and the URL path and S3 path corresponding to the file will be displayed; c. Clicking the \"Copy\" button of the S3 path, and paste the link into the corresponding attribute column in the data model  d. Uploading the entity data model: click \"+\" on the right side of the entity data model, the upload data model option box appears, and upload the edited csv file.  </p> <p>SAM, BAM, and CRAM are all different forms of the original SAM format defined to hold aligned (or more precisely, mapped) to high-throughput sequencing data. SAM stands for Sequence Alignment/Map format and is described in the standard specification http://samtools.github.io/hts-specs/. Both BAM and CRAM are compressed forms of SAM; BAM is a lossless compression, while CRAM can range from lossless to lossy, depending on how much compression you want to achieve (actually most). BAM and CRAM have the same information as their SAM equivalents and are structured in the same way; the difference between them is how the files themselves are encoded.</p> <p>Next we need to import the Cram-to-Bam workflow into Workspace\uff1a a. Click Import Workflow b. Select Custom Import  c. Enter the corresponding input items (for how to fill in the input, please read the Custom Import in the user manual)     - Git address\uff1a https://gitee.com/joy_lee/seq-format-conversion01     - Tag: v0.47     - Main path\uff1aCramToBam.wdl </p> <p>Now we have completed the import of the Cram to Bam pipeline workflow.</p>"},{"location":"TUTORIALS/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/GenomeanalysisusingGATK/#upload-data","title":"Upload data","text":"Usually\uff0c uploading the sample data is the first step in starting a workflow. In this practice, we need to transfer the data set to the storage bucket first, and make the corresponding data model. You have two methods to get data models:<ul> <li>You can download the sample-datasets file and reference-datasets file to the local according to the link below, then upload them to the storage bucket corresponding to the corresponding Workspace, and finally create a data model according to the S3 path corresponding to the file</li> <li>You can directly download the data model sample.csv file corresponding to this best practice. If you choose this method, you can start directly from step d.</li> </ul>"},{"location":"TUTORIALS/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/GenomeanalysisusingGATK/#import-workflow","title":"Import workflow","text":"Miracle Cloud uses workflows in Workflow Description Language (WDL) to analysis for genomics data. GATK tools that read input data all expect BAM files as the primary format. Some support the CRAM format, but we observed performance issues when working directly from CRAM files. So we first converted CRAM to BAM."},{"location":"TUTORIALS/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/GenomeanalysisusingGATK/#run-the-workflow","title":"Run the workflow","text":"We have just completed the import of the workflow, then we will run the imported workflow.     a. Associate environment: Click Environment Management, select Workflow - Associate Cluster, and select a cluster from the list of candidate clusters, click Associate.     b. Select the Cram-to-Bam workflow we just imported     c. Here we need to configure the run options and parameters . In the Run Options, select the data entity we just uploaded in the first step, and specify the entity data.      d. Configuring the input parameters, select the input parameters tab, and input according to the figure below. \"this\" indicates the csv file used in this analysis, and the content after \"this\" is the column name in the corresponding csv. When there are many input parameters, you can use the method of uploading a JSON file for quick import.     e. Configuring output parameters, use this.bai as the output attribute column of the bai file, this.bam as the attribute column of the bam file Through \"this.column\" , the analysis result is written back to the specified column of the data model; if there is no such column in the original table, a new column will be added; if the original table has a corresponding column, the new result will be directly filled or overwritten.     f. Click on the upper right corner to start the analysis, the workflow will be analyzed through the Cromwell pipeline engine, and the results can be viewed in the analysis history."},{"location":"TUTORIALS/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/GenomeanalysisusingGATK/#part-2-analyzing-the-bam-from-the-part-1","title":"Part 2: Analyzing the BAM from the part 1","text":"<p>In this chapter, we need to use the Bam and Bai files output in the first part for further analysis through the GATK workflow, and finally output the required gvcf files.</p> <ol> <li> <p>Run GATK workflow a. Select data: sample, still select NA12878 and my_sample_data, we will use the bam file output in the first part as the input for this part. b. Click the Input Parameters tab, select Hello_GATK.input_bam, and drop down in Attribute Values to select this.bam,After configuring all inputs as follows: c. Click the Output Parameters tab, where you can fill in the Attribute value of the output_gvcf variable. To write to the data table, first type \"this\". Then add a name to the attribute. The analysis will generate a column in the sample data table. d. Click the Start Analyzing button in the upper right corner to submit your workflow If your results are submitted correctly, the analysis status of the current workflow can be viewed in the job history. After a while you will see the analysis status change to analysis successful</p> </li> <li> </li> </ol>"},{"location":"TUTORIALS/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/GenomeanalysisusingGATK/#import-gatk-workflow","title":"Import GATK workflow","text":"We also need to import a workflow to take a genomic data file in BAM format and convert to gvcf format result files.Click Workflows on the left and click Import Workflow and fill in the corresponding input fields     a. Git address\uff1ahttps://gitee.com/joy_lee/gatk-pipeline     b. Tag: v0.34     c. Main path\uff1aHello-GATK.wdl"},{"location":"TUTORIALS/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/GenomeanalysisusingGATK/#view-job-history","title":"View Job History","text":"<p>After waiting for the analysis job to complete, you can click Job History on the left menu bar to view the output parameters, and download the gvcf file of the corresponding sample.So far, we have also completed the running of two workflows, completed the format conversion from Cram to Bam file, and performed analysis on the Bam sequence through GATK, and obtained the result gvcf file.</p>"},{"location":"TUTORIALS/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/GenomeanalysisusingGATK/#part-3-complete-workspace-overview","title":"Part 3: Complete Workspace Overview","text":"<p>If you have completed all the workflow and analysis job, you may also want to write a summary or introduction for your Workspace. You can click Dashboard on the left. Dashboard also integrates Jupyter Notebook. You can use MarkDown in Jupyter Notebook for document writing. When finished editing, click Save and Cancel editing.</p>"}]}